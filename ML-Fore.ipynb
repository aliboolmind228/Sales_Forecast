{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from joblib import dump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 42\n",
    "    cv_splits: int = 5\n",
    "    primary_metric: str = \"rmse\"\n",
    "    output_dir: str = \"output_ml\"\n",
    "    clean_data_filename: str = \"clean_preprocessed_dataset.csv\"\n",
    "    metrics_filename: str = \"validation_metrics.json\"\n",
    "    feature_importance_png: str = \"xgb_feature_importance.png\"\n",
    "    feature_importance_csv: str = \"xgb_feature_importance.csv\"\n",
    "    models_dir: str = \"models\"\n",
    "    forecast_filename: str = \"forecast_nov_dec_2025_weekdays.csv\"\n",
    "    zero_handling: str = \"keep\"\n",
    "    meta_scaler: bool = True\n",
    "\n",
    "CONFIG = Config()\n",
    "np.random.seed(CONFIG.seed)\n",
    "\n",
    "def ensure_output_dirs() -> None:\n",
    "    os.makedirs(CONFIG.output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(CONFIG.output_dir, CONFIG.models_dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core product mapping\n",
    "CORE_PRODUCT_MAP = {\n",
    "    \"Carrousel\": \"Carrousel\",\n",
    "    \"Reuzenrad\": \"Reuzenrad\",\n",
    "    \"Entree schaatsbaan\": \"Entree schaatsbaan\",\n",
    "    \"Schaatsverhuur\": \"Schaatsverhuur\",\n",
    "    \"Handschoenen\": \"Handschoenen\",\n",
    "}\n",
    "\n",
    "def parse_comma_list(value):\n",
    "    if pd.isna(value) or value is None:\n",
    "        return []\n",
    "    if isinstance(value, list):\n",
    "        return [str(v).strip() for v in value if str(v).strip()]\n",
    "    return [v.strip() for v in str(value).split(',') if v is not None and str(v).strip()]\n",
    "\n",
    "def map_to_core_product(name: str) -> str:\n",
    "    if pd.isna(name) or not str(name).strip():\n",
    "        return \"UNKNOWN\"\n",
    "    if name in CORE_PRODUCT_MAP:\n",
    "        return CORE_PRODUCT_MAP[name]\n",
    "    lower = str(name).lower()\n",
    "    if \"carrousel\" in lower:\n",
    "        return \"Carrousel\"\n",
    "    if \"reuzenrad\" in lower:\n",
    "        return \"Reuzenrad\"\n",
    "    if \"entree\" in lower or \"schaatsbaan\" in lower:\n",
    "        return \"Entree schaatsbaan\"\n",
    "    if \"schaatsverhuur\" in lower or \"verhuur\" in lower:\n",
    "        return \"Schaatsverhuur\"\n",
    "    if \"handschoen\" in lower:\n",
    "        return \"Handschoenen\"\n",
    "    return \"UNKNOWN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e8e3659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>products</th>\n",
       "      <th>productsVariants</th>\n",
       "      <th>productsVariantsIds</th>\n",
       "      <th>total</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>scannedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265</td>\n",
       "      <td>Carrousel</td>\n",
       "      <td>Kinderen</td>\n",
       "      <td>153</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2023-11-07 04:41:55.516001</td>\n",
       "      <td>2023-12-03 19:55:01.256086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266</td>\n",
       "      <td>Carrousel</td>\n",
       "      <td>Kinderen</td>\n",
       "      <td>153</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2023-11-07 04:43:41.545878</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>298</td>\n",
       "      <td>Handschoenen</td>\n",
       "      <td>Volwassen</td>\n",
       "      <td>159</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-11-08 04:00:19.312596</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id      products productsVariants productsVariantsIds  total  \\\n",
       "0  265     Carrousel         Kinderen                 153    3.5   \n",
       "1  266     Carrousel         Kinderen                 153    3.5   \n",
       "2  298  Handschoenen        Volwassen                 159    5.0   \n",
       "\n",
       "                    createdAt                   scannedAt  \n",
       "0  2023-11-07 04:41:55.516001  2023-12-03 19:55:01.256086  \n",
       "1  2023-11-07 04:43:41.545878                         NaN  \n",
       "2  2023-11-08 04:00:19.312596                         NaN  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and combine CSV files with schema harmonization\n",
    "schema = ['id','products','productsVariants','productsVariantsIds','total','createdAt','scannedAt']\n",
    "\n",
    "df_2023 = pd.read_csv('Final_product_2023_sp.csv')\n",
    "df_2024 = pd.read_csv('Final_product_2024_fp.csv')\n",
    "\n",
    "# Fix duplicated column in 2023\n",
    "if 'productsVariantsIds' not in df_2023.columns and 'productsVariants.1' in df_2023.columns:\n",
    "    df_2023 = df_2023.rename(columns={'productsVariants.1': 'productsVariantsIds'})\n",
    "if 'productsVariantsIds' in df_2023.columns and 'productsVariants.1' in df_2023.columns:\n",
    "    df_2023 = df_2023.drop(columns=['productsVariants.1'])\n",
    "for col in schema:\n",
    "    if col not in df_2023.columns:\n",
    "        df_2023[col] = np.nan\n",
    "\n",
    "# Fix for 2024\n",
    "if 'productsVariantsIds' not in df_2024.columns and 'productsVariants.1' in df_2024.columns:\n",
    "    df_2024 = df_2024.rename(columns={'productsVariants.1': 'productsVariantsIds'})\n",
    "if 'productsVariantsIds' in df_2024.columns and 'productsVariants.1' in df_2024.columns:\n",
    "    df_2024 = df_2024.drop(columns=['productsVariants.1'])\n",
    "for col in schema:\n",
    "    if col not in df_2024.columns:\n",
    "        df_2024[col] = np.nan\n",
    "\n",
    "df_2023 = df_2023[schema]\n",
    "df_2024 = df_2024[schema]\n",
    "\n",
    "df_raw = pd.concat([df_2023, df_2024], ignore_index=True)\n",
    "df_raw = df_raw.drop_duplicates()\n",
    "\n",
    "assert list(df_raw.columns) == schema, f\"Unexpected columns: {list(df_raw.columns)}\"\n",
    "df_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1cb0a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdAt</th>\n",
       "      <th>product_raw</th>\n",
       "      <th>product_core</th>\n",
       "      <th>variant_text</th>\n",
       "      <th>variant_id</th>\n",
       "      <th>total_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-07 04:41:55.516001</td>\n",
       "      <td>Carrousel</td>\n",
       "      <td>Carrousel</td>\n",
       "      <td>Kinderen</td>\n",
       "      <td>153</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-07 04:43:41.545878</td>\n",
       "      <td>Carrousel</td>\n",
       "      <td>Carrousel</td>\n",
       "      <td>Kinderen</td>\n",
       "      <td>153</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-08 04:00:19.312596</td>\n",
       "      <td>Handschoenen</td>\n",
       "      <td>Handschoenen</td>\n",
       "      <td>Volwassen</td>\n",
       "      <td>159</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    createdAt   product_raw  product_core variant_text  \\\n",
       "0  2023-11-07 04:41:55.516001     Carrousel     Carrousel     Kinderen   \n",
       "1  2023-11-07 04:43:41.545878     Carrousel     Carrousel     Kinderen   \n",
       "2  2023-11-08 04:00:19.312596  Handschoenen  Handschoenen    Volwassen   \n",
       "\n",
       "  variant_id  total_item  \n",
       "0        153         3.5  \n",
       "1        153         3.5  \n",
       "2        159         5.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand bundles and split revenue equally per item\n",
    "rows = []\n",
    "for _, row in df_raw.iterrows():\n",
    "    product_list = parse_comma_list(row[\"products\"])\n",
    "    variant_text_list = parse_comma_list(row[\"productsVariants\"])\n",
    "    variant_id_list = parse_comma_list(row[\"productsVariantsIds\"])\n",
    "    created_at = row[\"createdAt\"]\n",
    "    total = row[\"total\"]\n",
    "\n",
    "    count_items = 0\n",
    "    if len(variant_id_list) > 0:\n",
    "        count_items = len(variant_id_list)\n",
    "    elif len(product_list) > 0:\n",
    "        count_items = len(product_list)\n",
    "    elif len(variant_text_list) > 0:\n",
    "        count_items = len(variant_text_list)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    per_item_total = np.nan\n",
    "    if not pd.isna(total) and count_items > 0 and isinstance(total, (int, float, np.integer, np.floating)):\n",
    "        per_item_total = float(total) / float(count_items)\n",
    "\n",
    "    def normalize(lst):\n",
    "        lst = list(lst)\n",
    "        if len(lst) == count_items:\n",
    "            return lst\n",
    "        if len(lst) == 0:\n",
    "            return [None] * count_items\n",
    "        res = lst[:count_items]\n",
    "        if len(res) < count_items:\n",
    "            res += [None] * (count_items - len(res))\n",
    "        return res\n",
    "\n",
    "    product_list = normalize(product_list)\n",
    "    variant_text_list = normalize(variant_text_list)\n",
    "    variant_id_list = normalize(variant_id_list)\n",
    "\n",
    "    for p, vtext, vid in zip(product_list, variant_text_list, variant_id_list):\n",
    "        rows.append({\n",
    "            \"createdAt\": created_at,\n",
    "            \"product_raw\": p,\n",
    "            \"product_core\": map_to_core_product(p) if p is not None else \"UNKNOWN\",\n",
    "            \"variant_text\": vtext,\n",
    "            \"variant_id\": vid,\n",
    "            \"total_item\": per_item_total,\n",
    "        })\n",
    "\n",
    "expanded = pd.DataFrame(rows)\n",
    "expanded = expanded.dropna(subset=[\"variant_id\", \"total_item\", \"createdAt\"]).copy()\n",
    "expanded.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ed837ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Enhanced features: 25 total\n",
      "✓ Training samples: 26,107\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createdAt</th>\n",
       "      <th>product_raw</th>\n",
       "      <th>product_core</th>\n",
       "      <th>variant_id</th>\n",
       "      <th>total_item</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_mean_7d</th>\n",
       "      <th>rolling_mean_30d</th>\n",
       "      <th>rolling_mean_90d</th>\n",
       "      <th>ytd_cumsum</th>\n",
       "      <th>revenue_growth_rate</th>\n",
       "      <th>product_x_year</th>\n",
       "      <th>product_x_month</th>\n",
       "      <th>variant_x_year</th>\n",
       "      <th>variant_x_month</th>\n",
       "      <th>product_x_weekofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-11-07 04:41:55.516001</td>\n",
       "      <td>Carrousel</td>\n",
       "      <td>Carrousel</td>\n",
       "      <td>153</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10115</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-07 04:43:41.545878</td>\n",
       "      <td>Carrousel</td>\n",
       "      <td>Carrousel</td>\n",
       "      <td>153</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10115</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-11-08 04:00:19.312596</td>\n",
       "      <td>Handschoenen</td>\n",
       "      <td>Handschoenen</td>\n",
       "      <td>159</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4046</td>\n",
       "      <td>22</td>\n",
       "      <td>12138</td>\n",
       "      <td>66</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   createdAt   product_raw  product_core variant_id  \\\n",
       "0 2023-11-07 04:41:55.516001     Carrousel     Carrousel        153   \n",
       "1 2023-11-07 04:43:41.545878     Carrousel     Carrousel        153   \n",
       "2 2023-11-08 04:00:19.312596  Handschoenen  Handschoenen        159   \n",
       "\n",
       "   total_item  year  month  weekofyear  dayofweek  day  ...  rolling_mean_7d  \\\n",
       "0         3.5  2023     11          45          1    7  ...              3.5   \n",
       "1         3.5  2023     11          45          1    7  ...              3.5   \n",
       "2         5.0  2023     11          45          2    8  ...              5.0   \n",
       "\n",
       "   rolling_mean_30d  rolling_mean_90d  ytd_cumsum  revenue_growth_rate  \\\n",
       "0               3.5               3.5         3.5                  0.0   \n",
       "1               3.5               3.5         7.0                  0.0   \n",
       "2               5.0               5.0         5.0                  0.0   \n",
       "\n",
       "   product_x_year  product_x_month  variant_x_year  variant_x_month  \\\n",
       "0               0                0           10115               55   \n",
       "1               0                0           10115               55   \n",
       "2            4046               22           12138               66   \n",
       "\n",
       "   product_x_weekofyear  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                    90  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ENHANCED Temporal feature engineering with ADVANCED TREND FEATURES\n",
    "fe = expanded.copy()\n",
    "fe[\"createdAt\"] = pd.to_datetime(fe[\"createdAt\"], errors=\"coerce\", utc=False)\n",
    "fe = fe.dropna(subset=[\"createdAt\"]).copy()\n",
    "\n",
    "# Sort by date for proper time series features\n",
    "fe = fe.sort_values(\"createdAt\").reset_index(drop=True)\n",
    "\n",
    "# Replace zeros with small floor value (1e-3) before log transform\n",
    "fe[\"total_item\"] = fe[\"total_item\"].clip(lower=1e-3)\n",
    "\n",
    "fe[\"year\"] = fe[\"createdAt\"].dt.year\n",
    "fe[\"month\"] = fe[\"createdAt\"].dt.month\n",
    "fe[\"weekofyear\"] = fe[\"createdAt\"].dt.isocalendar().week.astype(int)\n",
    "fe[\"dayofweek\"] = fe[\"createdAt\"].dt.dayofweek\n",
    "fe[\"day\"] = fe[\"createdAt\"].dt.day\n",
    "fe[\"hour\"] = 12\n",
    "fe[\"is_weekend\"] = fe[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "fe[\"is_month_start\"] = fe[\"createdAt\"].dt.is_month_start.astype(int)\n",
    "fe[\"is_month_end\"] = fe[\"createdAt\"].dt.is_month_end.astype(int)\n",
    "\n",
    "fe[\"target_log1p\"] = np.log1p(fe[\"total_item\"])\n",
    "\n",
    "# Label encode before creating interaction features\n",
    "label_encoders: Dict[str, LabelEncoder] = {}\n",
    "for col in [\"product_core\", \"variant_id\"]:\n",
    "    le = LabelEncoder()\n",
    "    fe[f\"{col}_le\"] = le.fit_transform(fe[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 1. SHORT-TERM LAG FEATURES per product-variant\n",
    "fe[\"lag_7d\"] = fe.groupby([\"product_core\", \"variant_id\"])[\"total_item\"].shift(7).fillna(fe[\"total_item\"].median())\n",
    "fe[\"lag_30d\"] = fe.groupby([\"product_core\", \"variant_id\"])[\"total_item\"].shift(30).fillna(fe[\"total_item\"].median())\n",
    "\n",
    "# 2. LONG-TERM LAG FEATURES (new)\n",
    "fe[\"lag_90d\"] = fe.groupby([\"product_core\", \"variant_id\"])[\"total_item\"].shift(90).fillna(fe[\"total_item\"].median())\n",
    "fe[\"lag_365d\"] = fe.groupby([\"product_core\", \"variant_id\"])[\"total_item\"].shift(365).fillna(fe[\"total_item\"].median())\n",
    "\n",
    "# 3. ROLLING MEAN per product (7, 30, 90 day windows)\n",
    "fe[\"rolling_mean_7d\"] = fe.groupby(\"product_core\")[\"total_item\"].transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
    "fe[\"rolling_mean_30d\"] = fe.groupby(\"product_core\")[\"total_item\"].transform(lambda x: x.rolling(30, min_periods=1).mean())\n",
    "fe[\"rolling_mean_90d\"] = fe.groupby(\"product_core\")[\"total_item\"].transform(lambda x: x.rolling(90, min_periods=1).mean())\n",
    "\n",
    "# 4. YTD CUMULATIVE per product per year\n",
    "fe[\"ytd_cumsum\"] = fe.groupby([\"product_core\", \"year\"])[\"total_item\"].cumsum()\n",
    "\n",
    "# 5. REVENUE GROWTH RATE (YoY comparison)\n",
    "fe_yearly_avg = fe.groupby([\"product_core\", \"year\"])[\"total_item\"].mean().reset_index()\n",
    "fe_yearly_avg = fe_yearly_avg.sort_values([\"product_core\", \"year\"])\n",
    "fe_yearly_avg[\"revenue_growth_rate\"] = fe_yearly_avg.groupby(\"product_core\")[\"total_item\"].pct_change().fillna(0)\n",
    "fe = fe.merge(fe_yearly_avg[[\"product_core\", \"year\", \"revenue_growth_rate\"]], on=[\"product_core\", \"year\"], how=\"left\")\n",
    "fe[\"revenue_growth_rate\"] = fe[\"revenue_growth_rate\"].fillna(0)\n",
    "\n",
    "# 6. INTERACTION TERMS for trend modeling (enhanced)\n",
    "fe[\"product_x_year\"] = fe[\"product_core_le\"] * fe[\"year\"]\n",
    "fe[\"product_x_month\"] = fe[\"product_core_le\"] * fe[\"month\"]\n",
    "fe[\"variant_x_year\"] = fe[\"variant_id_le\"] * fe[\"year\"]\n",
    "fe[\"variant_x_month\"] = fe[\"variant_id_le\"] * fe[\"month\"]  # NEW\n",
    "fe[\"product_x_weekofyear\"] = fe[\"product_core_le\"] * fe[\"weekofyear\"]  # NEW\n",
    "\n",
    "fe = fe.drop(columns=[\"variant_text\"], errors=\"ignore\")\n",
    "\n",
    "# Filter out UNKNOWN products\n",
    "fe = fe[fe[\"product_core\"] != \"UNKNOWN\"].copy()\n",
    "\n",
    "feature_cols = [\n",
    "    \"product_core_le\",\"variant_id_le\",\"year\",\"month\",\"weekofyear\",\"dayofweek\",\"day\",\"hour\",\n",
    "    \"is_weekend\",\"is_month_start\",\"is_month_end\",\n",
    "    \"lag_7d\",\"lag_30d\",\"lag_90d\",\"lag_365d\",\n",
    "    \"rolling_mean_7d\",\"rolling_mean_30d\",\"rolling_mean_90d\",\n",
    "    \"ytd_cumsum\",\"revenue_growth_rate\",\n",
    "    \"product_x_year\",\"product_x_month\",\"variant_x_year\",\"variant_x_month\",\"product_x_weekofyear\"\n",
    "]\n",
    "\n",
    "target_col = \"target_log1p\"\n",
    "X = fe[feature_cols].to_numpy()\n",
    "y = fe[target_col].to_numpy()\n",
    "\n",
    "print(f\"✓ Enhanced features: {len(feature_cols)} total\")\n",
    "print(f\"✓ Training samples: {len(X):,}\")\n",
    "fe.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c896ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking model with OOF meta-features\n",
    "class StackingRegressorOOF(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, base_models, meta_model, cv_splits=5, use_meta_scaler=True, random_state=42):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.cv_splits = cv_splits\n",
    "        self.use_meta_scaler = use_meta_scaler\n",
    "        self.random_state = random_state\n",
    "        self.fitted_base_models_ = []\n",
    "        self.meta_scaler_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tscv = TimeSeriesSplit(n_splits=self.cv_splits)\n",
    "        oof_preds = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        for model_idx, base_model in enumerate(self.base_models):\n",
    "            for train_idx, valid_idx in tscv.split(X):\n",
    "                x_tr, x_va = X[train_idx], X[valid_idx]\n",
    "                y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "                m = clone(base_model)\n",
    "                if hasattr(m, 'random_state'):\n",
    "                    m.random_state = self.random_state\n",
    "                m.fit(x_tr, y_tr)\n",
    "                oof_preds[valid_idx, model_idx] = m.predict(x_va)\n",
    "\n",
    "        if self.use_meta_scaler:\n",
    "            self.meta_scaler_ = StandardScaler()\n",
    "            meta_X = self.meta_scaler_.fit_transform(oof_preds)\n",
    "        else:\n",
    "            meta_X = oof_preds\n",
    "        self.meta_model.fit(meta_X, y)\n",
    "\n",
    "        self.fitted_base_models_ = []\n",
    "        for base_model in self.base_models:\n",
    "            bm = clone(base_model)\n",
    "            if hasattr(bm, 'random_state'):\n",
    "                bm.random_state = self.random_state\n",
    "            bm.fit(X, y)\n",
    "            self.fitted_base_models_.append(bm)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        base_preds = np.column_stack([m.predict(X) for m in self.fitted_base_models_])\n",
    "        if self.use_meta_scaler and self.meta_scaler_ is not None:\n",
    "            base_preds = self.meta_scaler_.transform(base_preds)\n",
    "        return self.meta_model.predict(base_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00c56eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna optimization for XGBoost\n",
    "def optimize_xgb(X, y, n_trials=40):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1200),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.0, 10.0),\n",
    "            \"random_state\": CONFIG.seed,\n",
    "            \"tree_method\": \"hist\",\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "        tscv = TimeSeriesSplit(n_splits=CONFIG.cv_splits)\n",
    "        rmses = []\n",
    "        for train_idx, valid_idx in tscv.split(X):\n",
    "            x_tr, x_va = X[train_idx], X[valid_idx]\n",
    "            y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "            model.fit(x_tr, y_tr, verbose=False)\n",
    "            preds = model.predict(x_va)\n",
    "            rmse = math.sqrt(mean_squared_error(y_va, preds))\n",
    "            rmses.append(rmse)\n",
    "        return float(np.mean(rmses))\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    return study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81c7492d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 20:25:06,867] A new study created in memory with name: no-name-783e5da7-2e1e-4207-af5c-8f8cf5bf7ba8\n",
      "[I 2025-10-30 20:25:09,305] Trial 0 finished with value: 0.29919213099228825 and parameters: {'n_estimators': 474, 'max_depth': 6, 'learning_rate': 0.04146548112278191, 'subsample': 0.7287316852652026, 'colsample_bytree': 0.6888639331450189, 'gamma': 3.1200290192123163, 'min_child_weight': 6.6291921755874235}. Best is trial 0 with value: 0.29919213099228825.\n",
      "[I 2025-10-30 20:25:11,844] Trial 1 finished with value: 0.29974890416654665 and parameters: {'n_estimators': 521, 'max_depth': 4, 'learning_rate': 0.16651338697093232, 'subsample': 0.9614204094539761, 'colsample_bytree': 0.7050589217845095, 'gamma': 3.9987506754362965, 'min_child_weight': 1.6584469980350025}. Best is trial 0 with value: 0.29919213099228825.\n",
      "[I 2025-10-30 20:25:13,102] Trial 2 finished with value: 0.30043801740740034 and parameters: {'n_estimators': 263, 'max_depth': 3, 'learning_rate': 0.09999846772761123, 'subsample': 0.6358070406618762, 'colsample_bytree': 0.9946767170815629, 'gamma': 3.6983643430688424, 'min_child_weight': 9.14098612962386}. Best is trial 0 with value: 0.29919213099228825.\n",
      "[I 2025-10-30 20:25:14,827] Trial 3 finished with value: 0.3087726865407234 and parameters: {'n_estimators': 287, 'max_depth': 10, 'learning_rate': 0.09704618757785718, 'subsample': 0.6358301865860204, 'colsample_bytree': 0.8754031393238719, 'gamma': 1.4505887628065373, 'min_child_weight': 3.9221554152375813}. Best is trial 0 with value: 0.29919213099228825.\n",
      "[I 2025-10-30 20:25:19,476] Trial 4 finished with value: 0.30395476519974574 and parameters: {'n_estimators': 1188, 'max_depth': 5, 'learning_rate': 0.2546711892959106, 'subsample': 0.9372340130639696, 'colsample_bytree': 0.6190841676666727, 'gamma': 2.204168095499383, 'min_child_weight': 8.96199375829288}. Best is trial 0 with value: 0.29919213099228825.\n",
      "[I 2025-10-30 20:25:22,832] Trial 5 finished with value: 0.3043361494578837 and parameters: {'n_estimators': 481, 'max_depth': 8, 'learning_rate': 0.03577013844338745, 'subsample': 0.8959413332661921, 'colsample_bytree': 0.6448094641656972, 'gamma': 0.5220951647862959, 'min_child_weight': 1.607358168544859}. Best is trial 0 with value: 0.29919213099228825.\n",
      "[I 2025-10-30 20:25:28,548] Trial 6 finished with value: 0.3080720190598366 and parameters: {'n_estimators': 1155, 'max_depth': 4, 'learning_rate': 0.03959503133369895, 'subsample': 0.8504757468251983, 'colsample_bytree': 0.8316771957738527, 'gamma': 1.0140147540136875, 'min_child_weight': 4.599080953930916}. Best is trial 0 with value: 0.29919213099228825.\n",
      "[I 2025-10-30 20:25:31,149] Trial 7 finished with value: 0.2986118133263237 and parameters: {'n_estimators': 519, 'max_depth': 5, 'learning_rate': 0.04493612887077123, 'subsample': 0.9446074077058254, 'colsample_bytree': 0.6202225521397571, 'gamma': 3.910608990292065, 'min_child_weight': 6.724721047101335}. Best is trial 7 with value: 0.2986118133263237.\n",
      "[I 2025-10-30 20:25:32,779] Trial 8 finished with value: 0.29919771650034155 and parameters: {'n_estimators': 310, 'max_depth': 7, 'learning_rate': 0.08007711029484563, 'subsample': 0.9424221219762406, 'colsample_bytree': 0.706495597432063, 'gamma': 3.7564180260174114, 'min_child_weight': 3.4993550876188086}. Best is trial 7 with value: 0.2986118133263237.\n",
      "[I 2025-10-30 20:25:37,097] Trial 9 finished with value: 0.2991260585593043 and parameters: {'n_estimators': 633, 'max_depth': 9, 'learning_rate': 0.03297550420419422, 'subsample': 0.9805285170697958, 'colsample_bytree': 0.7303157056241487, 'gamma': 0.9875548182097965, 'min_child_weight': 0.6277487439836882}. Best is trial 7 with value: 0.2986118133263237.\n",
      "[I 2025-10-30 20:25:41,706] Trial 10 finished with value: 0.29702929487829866 and parameters: {'n_estimators': 891, 'max_depth': 6, 'learning_rate': 0.017669863493041157, 'subsample': 0.786983138218925, 'colsample_bytree': 0.9106763096286543, 'gamma': 4.956204645730505, 'min_child_weight': 6.511303489884158}. Best is trial 10 with value: 0.29702929487829866.\n",
      "[I 2025-10-30 20:25:46,901] Trial 11 finished with value: 0.29709651410338783 and parameters: {'n_estimators': 897, 'max_depth': 6, 'learning_rate': 0.012327319758490956, 'subsample': 0.7721630883426008, 'colsample_bytree': 0.9274702697020546, 'gamma': 4.907801854392602, 'min_child_weight': 6.617266095940066}. Best is trial 10 with value: 0.29702929487829866.\n",
      "[I 2025-10-30 20:25:52,806] Trial 12 finished with value: 0.29681995242215764 and parameters: {'n_estimators': 906, 'max_depth': 7, 'learning_rate': 0.011308625934400357, 'subsample': 0.7637254430769175, 'colsample_bytree': 0.9417962909259182, 'gamma': 4.895395017231011, 'min_child_weight': 7.082945916311896}. Best is trial 12 with value: 0.29681995242215764.\n",
      "[I 2025-10-30 20:25:58,426] Trial 13 finished with value: 0.2965869808825381 and parameters: {'n_estimators': 870, 'max_depth': 7, 'learning_rate': 0.010363634386210005, 'subsample': 0.7197435812270203, 'colsample_bytree': 0.9539131231396445, 'gamma': 4.761208992994784, 'min_child_weight': 7.33391200891383}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:26:04,101] Trial 14 finished with value: 0.29676909668837803 and parameters: {'n_estimators': 864, 'max_depth': 8, 'learning_rate': 0.010573071354388727, 'subsample': 0.7047689335020746, 'colsample_bytree': 0.9932826671955023, 'gamma': 4.454306422350305, 'min_child_weight': 7.793062465637003}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:26:08,196] Trial 15 finished with value: 0.2978593130091418 and parameters: {'n_estimators': 760, 'max_depth': 8, 'learning_rate': 0.023350740328077456, 'subsample': 0.6972238627744785, 'colsample_bytree': 0.9941580960953634, 'gamma': 2.61491100037307, 'min_child_weight': 8.312494827318263}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:26:14,001] Trial 16 finished with value: 0.29720861551707733 and parameters: {'n_estimators': 1028, 'max_depth': 9, 'learning_rate': 0.016990024789098633, 'subsample': 0.6893696023340553, 'colsample_bytree': 0.7965495401446635, 'gamma': 4.400404103697593, 'min_child_weight': 9.935510618681583}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:26:18,865] Trial 17 finished with value: 0.2973783055441216 and parameters: {'n_estimators': 758, 'max_depth': 8, 'learning_rate': 0.011077491998140147, 'subsample': 0.8463974206568675, 'colsample_bytree': 0.9657988817672777, 'gamma': 3.1403453357941267, 'min_child_weight': 5.415393629658617}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:26:24,186] Trial 18 finished with value: 0.29767319153292526 and parameters: {'n_estimators': 1023, 'max_depth': 10, 'learning_rate': 0.023899301469420774, 'subsample': 0.6958993382856246, 'colsample_bytree': 0.8693877783736123, 'gamma': 4.3674821616697646, 'min_child_weight': 8.007056083674055}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:26:28,234] Trial 19 finished with value: 0.2976644193234272 and parameters: {'n_estimators': 642, 'max_depth': 9, 'learning_rate': 0.016194969801632476, 'subsample': 0.6119038193973532, 'colsample_bytree': 0.77492296638929, 'gamma': 2.0739564892145745, 'min_child_weight': 7.81111931193267}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:26:32,585] Trial 20 finished with value: 0.2988980857191062 and parameters: {'n_estimators': 1020, 'max_depth': 7, 'learning_rate': 0.06805231101786192, 'subsample': 0.8226400250273518, 'colsample_bytree': 0.8985592584473496, 'gamma': 3.207159205131693, 'min_child_weight': 5.469409007963115}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:26:38,201] Trial 21 finished with value: 0.2971503412930607 and parameters: {'n_estimators': 888, 'max_depth': 7, 'learning_rate': 0.010046800117221272, 'subsample': 0.7399310399155099, 'colsample_bytree': 0.9434737077327111, 'gamma': 4.4935485799905726, 'min_child_weight': 7.199811565720728}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:26:47,353] Trial 22 finished with value: 0.2967452181213538 and parameters: {'n_estimators': 803, 'max_depth': 8, 'learning_rate': 0.01391379963515133, 'subsample': 0.742222104699311, 'colsample_bytree': 0.9571090086404626, 'gamma': 4.9749052904000255, 'min_child_weight': 8.924618858989291}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:27:02,825] Trial 23 finished with value: 0.29752012350296925 and parameters: {'n_estimators': 791, 'max_depth': 8, 'learning_rate': 0.02382257397221312, 'subsample': 0.6629518390768165, 'colsample_bytree': 0.9725804480823115, 'gamma': 4.434572924898973, 'min_child_weight': 9.90781462497355}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:27:14,317] Trial 24 finished with value: 0.2967428622476454 and parameters: {'n_estimators': 832, 'max_depth': 9, 'learning_rate': 0.014909690415457201, 'subsample': 0.7370596650603218, 'colsample_bytree': 0.8409817702739015, 'gamma': 4.1727897150084985, 'min_child_weight': 8.744821537025315}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:27:19,253] Trial 25 finished with value: 0.29688906675826765 and parameters: {'n_estimators': 684, 'max_depth': 9, 'learning_rate': 0.014321613086706942, 'subsample': 0.7450224872393064, 'colsample_bytree': 0.8480469196085687, 'gamma': 3.53026753589337, 'min_child_weight': 9.10697473821291}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:27:25,724] Trial 26 finished with value: 0.297045002444435 and parameters: {'n_estimators': 968, 'max_depth': 10, 'learning_rate': 0.02106187675048098, 'subsample': 0.8174872804644335, 'colsample_bytree': 0.8233889652938586, 'gamma': 4.960565727212225, 'min_child_weight': 9.1163382356092}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:27:31,695] Trial 27 finished with value: 0.29764325575438527 and parameters: {'n_estimators': 805, 'max_depth': 9, 'learning_rate': 0.014165687915097145, 'subsample': 0.6673555490544226, 'colsample_bytree': 0.8918746586823386, 'gamma': 4.050292225973126, 'min_child_weight': 8.556424109009331}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:27:38,159] Trial 28 finished with value: 0.29674992962529384 and parameters: {'n_estimators': 1096, 'max_depth': 8, 'learning_rate': 0.01967138634670947, 'subsample': 0.723928509797105, 'colsample_bytree': 0.7757578259259386, 'gamma': 2.6702371989755607, 'min_child_weight': 6.0158938021338475}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:27:42,116] Trial 29 finished with value: 0.2973141913900812 and parameters: {'n_estimators': 589, 'max_depth': 7, 'learning_rate': 0.02690171157508825, 'subsample': 0.7286374085794277, 'colsample_bytree': 0.9239496110514902, 'gamma': 4.227314603386452, 'min_child_weight': 7.319613570428504}. Best is trial 13 with value: 0.2965869808825381.\n",
      "[I 2025-10-30 20:27:46,905] Trial 30 finished with value: 0.2964614539900935 and parameters: {'n_estimators': 829, 'max_depth': 6, 'learning_rate': 0.02988510250409403, 'subsample': 0.7617764679539732, 'colsample_bytree': 0.9581020497592166, 'gamma': 4.679963293954102, 'min_child_weight': 9.655284545725921}. Best is trial 30 with value: 0.2964614539900935.\n",
      "[I 2025-10-30 20:27:52,783] Trial 31 finished with value: 0.2967081320110929 and parameters: {'n_estimators': 807, 'max_depth': 5, 'learning_rate': 0.029003937189290504, 'subsample': 0.758150140881988, 'colsample_bytree': 0.9542373070359877, 'gamma': 4.654454556784559, 'min_child_weight': 9.668568114040653}. Best is trial 30 with value: 0.2964614539900935.\n",
      "[I 2025-10-30 20:27:57,553] Trial 32 finished with value: 0.2965803517809797 and parameters: {'n_estimators': 958, 'max_depth': 5, 'learning_rate': 0.04887186696388862, 'subsample': 0.7893361625476871, 'colsample_bytree': 0.8626134966996956, 'gamma': 4.606400445860193, 'min_child_weight': 9.76971583321845}. Best is trial 30 with value: 0.2964614539900935.\n",
      "[I 2025-10-30 20:28:02,632] Trial 33 finished with value: 0.2966978230334167 and parameters: {'n_estimators': 978, 'max_depth': 5, 'learning_rate': 0.06737481887825117, 'subsample': 0.7931607434416279, 'colsample_bytree': 0.8779738814689652, 'gamma': 4.654653197296782, 'min_child_weight': 9.718879622866382}. Best is trial 30 with value: 0.2964614539900935.\n",
      "[I 2025-10-30 20:28:07,744] Trial 34 finished with value: 0.29814381145734664 and parameters: {'n_estimators': 948, 'max_depth': 4, 'learning_rate': 0.052967948890109244, 'subsample': 0.8014570897867137, 'colsample_bytree': 0.869674193498133, 'gamma': 3.445200376007185, 'min_child_weight': 9.73658407214041}. Best is trial 30 with value: 0.2964614539900935.\n",
      "[I 2025-10-30 20:28:13,185] Trial 35 finished with value: 0.2996800447119924 and parameters: {'n_estimators': 1108, 'max_depth': 5, 'learning_rate': 0.11366656412830771, 'subsample': 0.8724677657554865, 'colsample_bytree': 0.8903804520129517, 'gamma': 4.682272094453965, 'min_child_weight': 9.444728426661133}. Best is trial 30 with value: 0.2964614539900935.\n",
      "[I 2025-10-30 20:28:18,060] Trial 36 finished with value: 0.2970125378647415 and parameters: {'n_estimators': 958, 'max_depth': 3, 'learning_rate': 0.06496025149986144, 'subsample': 0.7913403114460907, 'colsample_bytree': 0.9164743542443194, 'gamma': 3.8676218813450363, 'min_child_weight': 8.283507677506705}. Best is trial 30 with value: 0.2964614539900935.\n",
      "[I 2025-10-30 20:28:23,241] Trial 37 finished with value: 0.29832800476440413 and parameters: {'n_estimators': 1075, 'max_depth': 6, 'learning_rate': 0.12910950496867854, 'subsample': 0.8927297724991119, 'colsample_bytree': 0.8589358722082012, 'gamma': 4.69091809850711, 'min_child_weight': 9.469547945781972}. Best is trial 30 with value: 0.2964614539900935.\n",
      "[I 2025-10-30 20:28:29,317] Trial 38 finished with value: 0.29774969886928143 and parameters: {'n_estimators': 977, 'max_depth': 4, 'learning_rate': 0.05114864533810944, 'subsample': 0.8237125861230112, 'colsample_bytree': 0.8104183211095551, 'gamma': 2.8985739028479354, 'min_child_weight': 7.6516799893813445}. Best is trial 30 with value: 0.2964614539900935.\n",
      "[I 2025-10-30 20:28:35,450] Trial 39 finished with value: 0.3736282135460912 and parameters: {'n_estimators': 733, 'max_depth': 5, 'learning_rate': 0.20194518248383808, 'subsample': 0.7784346214779656, 'colsample_bytree': 0.8870764863331226, 'gamma': 0.125101880593391, 'min_child_weight': 3.1789891660010645}. Best is trial 30 with value: 0.2964614539900935.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': 0.4077056636202312,\n",
       " 'mae': 0.3158330251988017,\n",
       " 'r2': 0.16553308110966442,\n",
       " 'mape': 13.034013335115002}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate base models + stacking meta-learner\n",
    "best_xgb_params = optimize_xgb(X, y, n_trials=40)\n",
    "\n",
    "xgb = XGBRegressor(**best_xgb_params, random_state=CONFIG.seed, tree_method=\"hist\")\n",
    "lgbm = LGBMRegressor(n_estimators=600, learning_rate=0.05, max_depth=-1, subsample=0.8, colsample_bytree=0.8, random_state=CONFIG.seed)\n",
    "rf = RandomForestRegressor(n_estimators=600, max_depth=None, random_state=CONFIG.seed, n_jobs=-1)\n",
    "\n",
    "meta = Ridge(alpha=1.0, random_state=CONFIG.seed)\n",
    "stack = StackingRegressorOOF(base_models=[xgb, lgbm, rf], meta_model=meta, cv_splits=CONFIG.cv_splits, use_meta_scaler=CONFIG.meta_scaler, random_state=CONFIG.seed)\n",
    "stack.fit(X, y)\n",
    "\n",
    "# Cross-validated metrics\n",
    "metrics_list = []\n",
    "tscv = TimeSeriesSplit(n_splits=CONFIG.cv_splits)\n",
    "for train_idx, valid_idx in tscv.split(X):\n",
    "    x_tr, x_va = X[train_idx], X[valid_idx]\n",
    "    y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "    stack_cv = StackingRegressorOOF(base_models=[xgb, lgbm, rf], meta_model=Ridge(alpha=1.0, random_state=CONFIG.seed), cv_splits=CONFIG.cv_splits, use_meta_scaler=CONFIG.meta_scaler, random_state=CONFIG.seed)\n",
    "    stack_cv.fit(x_tr, y_tr)\n",
    "    preds_va = stack_cv.predict(x_va)\n",
    "    rmse = math.sqrt(mean_squared_error(y_va, preds_va))\n",
    "    mae = mean_absolute_error(y_va, preds_va)\n",
    "    r2 = r2_score(y_va, preds_va)\n",
    "    mape = float(np.mean(np.abs((np.expm1(y_va) - np.expm1(preds_va)) / np.clip(np.expm1(y_va), 1e-6, None))))\n",
    "    metrics_list.append({\"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"mape\": mape})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics = {\"rmse\": float(metrics_df[\"rmse\"].mean()), \"mae\": float(metrics_df[\"mae\"].mean()), \"r2\": float(metrics_df[\"r2\"].mean()), \"mape\": float(metrics_df[\"mape\"].mean())}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3160938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rolling_mean_30d</td>\n",
       "      <td>0.415746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rolling_mean_7d</td>\n",
       "      <td>0.217473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rolling_mean_90d</td>\n",
       "      <td>0.100301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>product_x_year</td>\n",
       "      <td>0.064214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>product_core_le</td>\n",
       "      <td>0.040275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lag_7d</td>\n",
       "      <td>0.021718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>variant_id_le</td>\n",
       "      <td>0.020067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>product_x_month</td>\n",
       "      <td>0.018259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ytd_cumsum</td>\n",
       "      <td>0.017836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lag_30d</td>\n",
       "      <td>0.017556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature  importance\n",
       "0  rolling_mean_30d    0.415746\n",
       "1   rolling_mean_7d    0.217473\n",
       "2  rolling_mean_90d    0.100301\n",
       "3    product_x_year    0.064214\n",
       "4   product_core_le    0.040275\n",
       "5            lag_7d    0.021718\n",
       "6     variant_id_le    0.020067\n",
       "7   product_x_month    0.018259\n",
       "8        ytd_cumsum    0.017836\n",
       "9           lag_30d    0.017556"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost feature importance\n",
    "xgb_fitted = None\n",
    "for m in stack.fitted_base_models_:\n",
    "    if isinstance(m, XGBRegressor):\n",
    "        xgb_fitted = m\n",
    "        break\n",
    "if xgb_fitted is None:\n",
    "    xgb_fitted = xgb.fit(X, y)\n",
    "\n",
    "importance = pd.DataFrame({\"feature\": feature_cols, \"importance\": xgb_fitted.feature_importances_}).sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "importance.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96470a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All outputs saved to: output_ml\n"
     ]
    }
   ],
   "source": [
    "# Save artifacts\n",
    "ensure_output_dirs()\n",
    "\n",
    "fe.to_csv(os.path.join(CONFIG.output_dir, CONFIG.clean_data_filename), index=False)\n",
    "\n",
    "model_dir = os.path.join(CONFIG.output_dir, CONFIG.models_dir)\n",
    "dump(stack, os.path.join(model_dir, \"stacking_model.joblib\"))\n",
    "dump(label_encoders, os.path.join(model_dir, \"label_encoders.joblib\"))\n",
    "\n",
    "with open(os.path.join(CONFIG.output_dir, CONFIG.metrics_filename), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "importance.to_csv(os.path.join(CONFIG.output_dir, CONFIG.feature_importance_csv), index=False)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=importance.head(20), x=\"importance\", y=\"feature\", orient=\"h\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG.output_dir, CONFIG.feature_importance_png))\n",
    "plt.close()\n",
    "\n",
    "forecast_items.to_csv(os.path.join(CONFIG.output_dir, \"forecast_items_nov_dec_2025.csv\"), index=False)\n",
    "forecast_daily.to_csv(os.path.join(CONFIG.output_dir, CONFIG.forecast_filename), index=False)\n",
    "\n",
    "print(\"✓ All outputs saved to:\", CONFIG.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c6be074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Overall historical growth (2023→2024): 42.52%\n",
      "✓ Product-level growth rates:\n",
      "  - Carrousel: -63.52%\n",
      "  - Handschoenen: -100.00%\n",
      "  - Reuzenrad: 48.60%\n",
      "  - Entree schaatsbaan: 38.90%\n",
      "  - Schaatsverhuur: 48.83%\n",
      "  - UNKNOWN: 39.20%\n"
     ]
    }
   ],
   "source": [
    "# META-TREND CORRECTOR: Calculate historical growth rates\n",
    "fe_2023 = fe[fe[\"year\"] == 2023]\n",
    "fe_2024 = fe[fe[\"year\"] == 2024]\n",
    "\n",
    "total_2023 = fe_2023[\"total_item\"].sum()\n",
    "total_2024 = fe_2024[\"total_item\"].sum()\n",
    "historical_growth_rate = (total_2024 - total_2023) / total_2023 if total_2023 > 0 else 0.20\n",
    "\n",
    "# Product-level growth rates\n",
    "product_growth = {}\n",
    "for prod in fe[\"product_core\"].unique():\n",
    "    p_2023 = fe_2023[fe_2023[\"product_core\"] == prod][\"total_item\"].sum()\n",
    "    p_2024 = fe_2024[fe_2024[\"product_core\"] == prod][\"total_item\"].sum()\n",
    "    if p_2023 > 0:\n",
    "        product_growth[prod] = (p_2024 - p_2023) / p_2023\n",
    "    else:\n",
    "        product_growth[prod] = historical_growth_rate\n",
    "\n",
    "print(f\"✓ Overall historical growth (2023→2024): {historical_growth_rate:.2%}\")\n",
    "print(f\"✓ Product-level growth rates:\")\n",
    "for prod, rate in product_growth.items():\n",
    "    print(f\"  - {prod}: {rate:.2%}\")\n",
    "\n",
    "# BUSINESS GROWTH CORRECTION PARAMETERS\n",
    "# Based on historical analysis, apply 19-21.5% growth for 2025\n",
    "BUSINESS_GROWTH_MIN = 0.19\n",
    "BUSINESS_GROWTH_MAX = 0.215\n",
    "BUSINESS_GROWTH_TARGET = 0.20  # 20% target growth\n",
    "\n",
    "print(f\"\\n✓ Business Growth Correction Range: {BUSINESS_GROWTH_MIN:.1%} - {BUSINESS_GROWTH_MAX:.1%}\")\n",
    "print(f\"✓ Target Growth Rate: {BUSINESS_GROWTH_TARGET:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca7861ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (84) does not match length of index (3612)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# LONG-TERM LAGS: Use 90d and 365d baseline\u001b[39;00m\n\u001b[32m     38\u001b[39m forecast_items[\u001b[33m\"\u001b[39m\u001b[33mlag_90d\u001b[39m\u001b[33m\"\u001b[39m] = forecast_items[\u001b[33m\"\u001b[39m\u001b[33mlag_avg\u001b[39m\u001b[33m\"\u001b[39m].fillna(fe[\u001b[33m\"\u001b[39m\u001b[33mtotal_item\u001b[39m\u001b[33m\"\u001b[39m].median())\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mforecast_items\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlag_365d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m = fe.groupby([\u001b[33m\"\u001b[39m\u001b[33mproduct_core\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvariant_id\u001b[39m\u001b[33m\"\u001b[39m])[\u001b[33m\"\u001b[39m\u001b[33mtotal_item\u001b[39m\u001b[33m\"\u001b[39m].mean().reset_index().merge(\n\u001b[32m     40\u001b[39m     forecast_items[[\u001b[33m\"\u001b[39m\u001b[33mproduct_core\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvariant_id\u001b[39m\u001b[33m\"\u001b[39m]].drop_duplicates(), on=[\u001b[33m\"\u001b[39m\u001b[33mproduct_core\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvariant_id\u001b[39m\u001b[33m\"\u001b[39m], how=\u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m )[\u001b[33m\"\u001b[39m\u001b[33mtotal_item\u001b[39m\u001b[33m\"\u001b[39m].fillna(fe[\u001b[33m\"\u001b[39m\u001b[33mtotal_item\u001b[39m\u001b[33m\"\u001b[39m].median()).values[:\u001b[38;5;28mlen\u001b[39m(forecast_items)]\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# ROLLING MEANS: Use 2024 averages with growth factor\u001b[39;00m\n\u001b[32m     44\u001b[39m rolling_baseline = fe[(fe[\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[32m2024\u001b[39m) & (fe[\u001b[33m\"\u001b[39m\u001b[33mcreatedAt\u001b[39m\u001b[33m\"\u001b[39m].dt.month.isin([\u001b[32m11\u001b[39m, \u001b[32m12\u001b[39m]))].groupby(\u001b[33m\"\u001b[39m\u001b[33mproduct_core\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[33m\"\u001b[39m\u001b[33mtotal_item\u001b[39m\u001b[33m\"\u001b[39m].mean().reset_index().rename(columns={\u001b[33m\"\u001b[39m\u001b[33mtotal_item\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrolling_avg\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4316\u001b[39m, in \u001b[36mDataFrame.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4313\u001b[39m     \u001b[38;5;28mself\u001b[39m._setitem_array([key], value)\n\u001b[32m   4314\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4315\u001b[39m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4316\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4529\u001b[39m, in \u001b[36mDataFrame._set_item\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4520\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4521\u001b[39m \u001b[33;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[32m   4522\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4527\u001b[39m \u001b[33;03m    ensure homogeneity.\u001b[39;00m\n\u001b[32m   4528\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4529\u001b[39m     value, refs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4531\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4532\u001b[39m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns\n\u001b[32m   4533\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m value.ndim == \u001b[32m1\u001b[39m\n\u001b[32m   4534\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value.dtype, ExtensionDtype)\n\u001b[32m   4535\u001b[39m     ):\n\u001b[32m   4536\u001b[39m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[32m   4537\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.is_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5273\u001b[39m, in \u001b[36mDataFrame._sanitize_column\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   5270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m.index)\n\u001b[32m   5272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[32m-> \u001b[39m\u001b[32m5273\u001b[39m     \u001b[43mcom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5274\u001b[39m arr = sanitize_array(value, \u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   5275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   5276\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[32m   5277\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m value.dtype == \u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5280\u001b[39m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[32m   5281\u001b[39m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[39m, in \u001b[36mrequire_length_match\u001b[39m\u001b[34m(data, index)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[33;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) != \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    574\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLength of values \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    575\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdoes not match length of index \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Length of values (84) does not match length of index (3612)"
     ]
    }
   ],
   "source": [
    "# ENHANCED Build weekday-only forecast calendar for Nov–Dec 2025\n",
    "from itertools import product as cart_prod\n",
    "\n",
    "calendar = pd.date_range(start=\"2025-11-01\", end=\"2025-12-31\", freq=\"D\")\n",
    "cal_df = pd.DataFrame({\"createdAt\": calendar})\n",
    "cal_df[\"dow\"] = cal_df[\"createdAt\"].dt.dayofweek\n",
    "cal_df = cal_df[cal_df[\"dow\"] < 5].copy()\n",
    "\n",
    "a_prod = list(label_encoders[\"product_core\"].classes_)\n",
    "a_var = list(label_encoders[\"variant_id\"].classes_)\n",
    "\n",
    "# Filter out UNKNOWN from product list\n",
    "a_prod = [p for p in a_prod if p != \"UNKNOWN\"]\n",
    "\n",
    "grid = list(cart_prod(cal_df[\"createdAt\"], a_prod, a_var))\n",
    "forecast_items = pd.DataFrame(grid, columns=[\"createdAt\", \"product_core\", \"variant_id\"])\n",
    "\n",
    "forecast_items[\"createdAt\"] = pd.to_datetime(forecast_items[\"createdAt\"], utc=False)\n",
    "forecast_items[\"year\"] = forecast_items[\"createdAt\"].dt.year\n",
    "forecast_items[\"month\"] = forecast_items[\"createdAt\"].dt.month\n",
    "forecast_items[\"weekofyear\"] = forecast_items[\"createdAt\"].dt.isocalendar().week.astype(int)\n",
    "forecast_items[\"dayofweek\"] = forecast_items[\"createdAt\"].dt.dayofweek\n",
    "forecast_items[\"day\"] = forecast_items[\"createdAt\"].dt.day\n",
    "forecast_items[\"hour\"] = 12\n",
    "forecast_items[\"is_weekend\"] = 0\n",
    "forecast_items[\"is_month_start\"] = forecast_items[\"createdAt\"].dt.is_month_start.astype(int)\n",
    "forecast_items[\"is_month_end\"] = forecast_items[\"createdAt\"].dt.is_month_end.astype(int)\n",
    "\n",
    "# Label encode\n",
    "forecast_items[\"product_core_le\"] = label_encoders[\"product_core\"].transform(forecast_items[\"product_core\"].astype(str))\n",
    "forecast_items[\"variant_id_le\"] = label_encoders[\"variant_id\"].transform(forecast_items[\"variant_id\"].astype(str))\n",
    "\n",
    "# SHORT-TERM LAGS: Use 2024 Nov-Dec averages per product-variant\n",
    "lag_baseline = fe[fe[\"createdAt\"].dt.month.isin([11, 12]) & (fe[\"year\"] == 2024)].groupby([\"product_core\", \"variant_id\"])[\"total_item\"].mean().reset_index().rename(columns={\"total_item\": \"lag_avg\"})\n",
    "forecast_items = forecast_items.merge(lag_baseline, on=[\"product_core\", \"variant_id\"], how=\"left\")\n",
    "forecast_items[\"lag_7d\"] = forecast_items[\"lag_avg\"].fillna(fe[\"total_item\"].median())\n",
    "forecast_items[\"lag_30d\"] = forecast_items[\"lag_avg\"].fillna(fe[\"total_item\"].median())\n",
    "\n",
    "# LONG-TERM LAGS: Use historical averages per product-variant\n",
    "long_lag_baseline = fe.groupby([\"product_core\", \"variant_id\"])[\"total_item\"].mean().reset_index().rename(columns={\"total_item\": \"long_lag_avg\"})\n",
    "forecast_items = forecast_items.merge(long_lag_baseline, on=[\"product_core\", \"variant_id\"], how=\"left\")\n",
    "forecast_items[\"lag_90d\"] = forecast_items[\"long_lag_avg\"].fillna(fe[\"total_item\"].median())\n",
    "forecast_items[\"lag_365d\"] = forecast_items[\"long_lag_avg\"].fillna(fe[\"total_item\"].median())\n",
    "\n",
    "# ROLLING MEANS: Use 2024 averages with growth factor per product\n",
    "rolling_baseline = fe[(fe[\"year\"] == 2024) & (fe[\"createdAt\"].dt.month.isin([11, 12]))].groupby(\"product_core\")[\"total_item\"].mean().reset_index().rename(columns={\"total_item\": \"rolling_avg\"})\n",
    "forecast_items = forecast_items.merge(rolling_baseline, on=\"product_core\", how=\"left\")\n",
    "forecast_items[\"rolling_mean_7d\"] = forecast_items[\"rolling_avg\"].fillna(fe[\"total_item\"].median())\n",
    "forecast_items[\"rolling_mean_30d\"] = forecast_items[\"rolling_avg\"].fillna(fe[\"total_item\"].median())\n",
    "forecast_items[\"rolling_mean_90d\"] = forecast_items[\"rolling_avg\"].fillna(fe[\"total_item\"].median())\n",
    "\n",
    "# YTD_CUMSUM: Project 2025 based on 2024 pattern per product\n",
    "ytd_baseline = fe[(fe[\"year\"] == 2024) & (fe[\"createdAt\"].dt.month.isin([11, 12]))].groupby(\"product_core\")[\"total_item\"].sum().reset_index().rename(columns={\"total_item\": \"ytd_2024\"})\n",
    "forecast_items = forecast_items.merge(ytd_baseline, on=\"product_core\", how=\"left\")\n",
    "forecast_items[\"ytd_cumsum\"] = forecast_items[\"ytd_2024\"].fillna(0)\n",
    "\n",
    "# REVENUE GROWTH RATE: Use historical product-level growth\n",
    "forecast_items[\"revenue_growth_rate\"] = forecast_items[\"product_core\"].map(product_growth).fillna(historical_growth_rate)\n",
    "\n",
    "# INTERACTION TERMS (all 5 new features)\n",
    "forecast_items[\"product_x_year\"] = forecast_items[\"product_core_le\"] * forecast_items[\"year\"]\n",
    "forecast_items[\"product_x_month\"] = forecast_items[\"product_core_le\"] * forecast_items[\"month\"]\n",
    "forecast_items[\"variant_x_year\"] = forecast_items[\"variant_id_le\"] * forecast_items[\"year\"]\n",
    "forecast_items[\"variant_x_month\"] = forecast_items[\"variant_id_le\"] * forecast_items[\"month\"]\n",
    "forecast_items[\"product_x_weekofyear\"] = forecast_items[\"product_core_le\"] * forecast_items[\"weekofyear\"]\n",
    "\n",
    "# Clean up temporary columns\n",
    "forecast_items = forecast_items.drop(columns=[\"lag_avg\", \"long_lag_avg\", \"rolling_avg\", \"ytd_2024\"], errors=\"ignore\")\n",
    "\n",
    "# Verify all 25 features are present\n",
    "missing_features = set(feature_cols) - set(forecast_items.columns)\n",
    "if missing_features:\n",
    "    raise ValueError(f\"Missing features in forecast_items: {missing_features}\")\n",
    "\n",
    "print(f\"✓ All 25 features populated for {len(forecast_items):,} forecast rows\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# STEP 1: MODEL BASELINE PREDICTION\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "Xf = forecast_items[feature_cols].to_numpy()\n",
    "pred_log = stack.predict(Xf)\n",
    "pred_total_baseline = np.expm1(pred_log)\n",
    "\n",
    "forecast_items[\"model_baseline\"] = pred_total_baseline\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# STEP 2: BUSINESS GROWTH CORRECTION LAYER (19-21.5%)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# Apply realistic business growth range based on 2023→2024 trend\n",
    "np.random.seed(42)  # For reproducibility\n",
    "forecast_items[\"business_growth_factor\"] = np.random.uniform(\n",
    "    1 + BUSINESS_GROWTH_MIN, \n",
    "    1 + BUSINESS_GROWTH_MAX, \n",
    "    size=len(forecast_items)\n",
    ")\n",
    "\n",
    "# Alternative: Use fixed 20% growth\n",
    "# forecast_items[\"business_growth_factor\"] = 1.20\n",
    "\n",
    "forecast_items[\"forecast_adjusted\"] = forecast_items[\"model_baseline\"] * forecast_items[\"business_growth_factor\"]\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# STEP 3: AGGREGATE DAILY FORECASTS\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "daily_agg = forecast_items.groupby(forecast_items[\"createdAt\"].dt.date).agg({\n",
    "    \"model_baseline\": \"sum\",\n",
    "    \"forecast_adjusted\": \"sum\"\n",
    "}).reset_index()\n",
    "daily_agg.columns = [\"date\", \"baseline_total\", \"adjusted_total\"]\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# STEP 4: SUMMARY METRICS\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "baseline_sum = daily_agg[\"baseline_total\"].sum()\n",
    "adjusted_sum = daily_agg[\"adjusted_total\"].sum()\n",
    "applied_growth = (adjusted_sum - baseline_sum) / baseline_sum\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 2025 FORECAST SUMMARY (Nov-Dec Weekdays)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model Baseline:          €{baseline_sum:,.2f}\")\n",
    "print(f\"Business-Adjusted:       €{adjusted_sum:,.2f}\")\n",
    "print(f\"Applied Growth:          {applied_growth:.2%}\")\n",
    "print(f\"Target Range:            {BUSINESS_GROWTH_MIN:.1%} - {BUSINESS_GROWTH_MAX:.1%}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n🎯 Historical Reference:\")\n",
    "print(f\"  • 2023 Nov-Dec (Mon-Fri): €29,462.42\")\n",
    "print(f\"  • 2024 Nov-Dec (Mon-Fri): €33,869.54\")\n",
    "print(f\"  • 2025 Forecast (Adjusted): €{adjusted_sum:,.2f}\")\n",
    "print(f\"  • Growth (2024→2025): {((adjusted_sum / 33869.54) - 1):.2%}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store for use in later cells\n",
    "forecast_daily = daily_agg.copy()\n",
    "forecast_daily.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb3fa4",
   "metadata": {},
   "source": [
    "---\n",
    "## 🔬 STANDALONE MODELS (For Comparison)\n",
    "Train individual models separately to compare against the stacking ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "133b1f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Standalone XGBoost...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 25, got 19",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     21\u001b[39m xgb_final_metrics = {\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mXGBoost_Standalone\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(xgb_metrics_df[\u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m].mean()),\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(xgb_metrics_df[\u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m].mean()),\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(xgb_metrics_df[\u001b[33m\"\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m\"\u001b[39m].mean())\n\u001b[32m     26\u001b[39m }\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Forecast with XGBoost\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m pred_log_xgb = \u001b[43mxgb_standalone\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m pred_total_xgb = np.expm1(pred_log_xgb)\n\u001b[32m     31\u001b[39m forecast_items[\u001b[33m\"\u001b[39m\u001b[33mpred_xgb_raw\u001b[39m\u001b[33m\"\u001b[39m] = pred_total_xgb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1327\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmargin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1336\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:2677\u001b[39m, in \u001b[36mBooster.inplace_predict\u001b[39m\u001b[34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[39m\n\u001b[32m   2673\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   2674\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2675\u001b[39m         )\n\u001b[32m   2676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data.shape) != \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_features() != data.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m-> \u001b[39m\u001b[32m2677\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2678\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2679\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2680\u001b[39m         )\n\u001b[32m   2682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[32m   2683\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _ensure_np_dtype\n",
      "\u001b[31mValueError\u001b[39m: Feature shape mismatch, expected: 25, got 19"
     ]
    }
   ],
   "source": [
    "# STANDALONE MODEL 1: XGBoost (Optuna-tuned)\n",
    "print(\"Training Standalone XGBoost...\")\n",
    "xgb_standalone = XGBRegressor(**best_xgb_params, random_state=CONFIG.seed, tree_method=\"hist\")\n",
    "xgb_standalone.fit(X, y)\n",
    "\n",
    "# Cross-validate\n",
    "xgb_metrics = []\n",
    "tscv = TimeSeriesSplit(n_splits=CONFIG.cv_splits)\n",
    "for train_idx, valid_idx in tscv.split(X):\n",
    "    x_tr, x_va = X[train_idx], X[valid_idx]\n",
    "    y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "    xgb_cv = XGBRegressor(**best_xgb_params, random_state=CONFIG.seed, tree_method=\"hist\")\n",
    "    xgb_cv.fit(x_tr, y_tr, verbose=False)\n",
    "    preds_va = xgb_cv.predict(x_va)\n",
    "    rmse = math.sqrt(mean_squared_error(y_va, preds_va))\n",
    "    mae = mean_absolute_error(y_va, preds_va)\n",
    "    r2 = r2_score(y_va, preds_va)\n",
    "    xgb_metrics.append({\"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "\n",
    "xgb_metrics_df = pd.DataFrame(xgb_metrics)\n",
    "xgb_final_metrics = {\n",
    "    \"model\": \"XGBoost_Standalone\",\n",
    "    \"rmse\": float(xgb_metrics_df[\"rmse\"].mean()),\n",
    "    \"mae\": float(xgb_metrics_df[\"mae\"].mean()),\n",
    "    \"r2\": float(xgb_metrics_df[\"r2\"].mean())\n",
    "}\n",
    "\n",
    "# Forecast with XGBoost\n",
    "pred_log_xgb = xgb_standalone.predict(Xf)\n",
    "pred_total_xgb = np.expm1(pred_log_xgb)\n",
    "forecast_items[\"pred_xgb_raw\"] = pred_total_xgb\n",
    "forecast_items[\"pred_xgb\"] = forecast_items[\"pred_xgb_raw\"] * forecast_items[\"growth_factor\"]\n",
    "\n",
    "forecast_daily_xgb = forecast_items.groupby(forecast_items[\"createdAt\"].dt.date)[\"pred_xgb\"].sum().reset_index().rename(columns={\"createdAt\": \"date\", \"pred_xgb\": \"forecast_total_xgb\"})\n",
    "\n",
    "print(f\"✓ XGBoost Metrics: RMSE={xgb_final_metrics['rmse']:.4f}, MAE={xgb_final_metrics['mae']:.4f}, R²={xgb_final_metrics['r2']:.4f}\")\n",
    "print(f\"✓ XGBoost Forecast (Nov-Dec 2025): €{forecast_daily_xgb['forecast_total_xgb'].sum():,.2f}\")\n",
    "xgb_final_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3ae6c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Standalone LightGBM...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 19 features, but LGBMRegressor is expecting 25 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     20\u001b[39m lgbm_final_metrics = {\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLightGBM_Standalone\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(lgbm_metrics_df[\u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m].mean()),\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(lgbm_metrics_df[\u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m].mean()),\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(lgbm_metrics_df[\u001b[33m\"\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m\"\u001b[39m].mean())\n\u001b[32m     25\u001b[39m }\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Forecast with LightGBM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m pred_log_lgbm = \u001b[43mlgbm_standalone\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m pred_total_lgbm = np.expm1(pred_log_lgbm)\n\u001b[32m     30\u001b[39m forecast_items[\u001b[33m\"\u001b[39m\u001b[33mpred_lgbm_raw\u001b[39m\u001b[33m\"\u001b[39m] = pred_total_lgbm\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\sklearn.py:1108\u001b[39m, in \u001b[36mLGBMModel.predict\u001b[39m\u001b[34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[39m\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LGBMNotFittedError(\u001b[33m\"\u001b[39m\u001b[33mEstimator not fitted, call fit before exploiting the model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (pd_DataFrame, dt_DataTable)):\n\u001b[32m-> \u001b[39m\u001b[32m1108\u001b[39m     X = \u001b[43m_LGBMValidateData\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# 'y' being omitted = run scikit-learn's check_array() instead of check_X_y()\u001b[39;49;00m\n\u001b[32m   1112\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#\u001b[39;49;00m\n\u001b[32m   1113\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Prevent scikit-learn from deleting or modifying attributes like 'feature_names_in_' and 'n_features_in_'.\u001b[39;49;00m\n\u001b[32m   1114\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# These shouldn't be changed at predict() time.\u001b[39;49;00m\n\u001b[32m   1115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# allow any input type (this validation is done further down, in lgb.Dataset())\u001b[39;49;00m\n\u001b[32m   1117\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# do not raise an error if Inf of NaN values are found (LightGBM handles these internally)\u001b[39;49;00m\n\u001b[32m   1119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# raise an error on 0-row inputs\u001b[39;49;00m\n\u001b[32m   1121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# retrieve original params that possibly can be used in both training and prediction\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;66;03m# and then overwrite them (considering aliases) with params that were passed directly in prediction\u001b[39;00m\n\u001b[32m   1125\u001b[39m predict_params = \u001b[38;5;28mself\u001b[39m._process_params(stage=\u001b[33m\"\u001b[39m\u001b[33mpredict\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 19 features, but LGBMRegressor is expecting 25 features as input."
     ]
    }
   ],
   "source": [
    "# STANDALONE MODEL 2: LightGBM\n",
    "print(\"Training Standalone LightGBM...\")\n",
    "lgbm_standalone = LGBMRegressor(n_estimators=600, learning_rate=0.05, max_depth=-1, subsample=0.8, colsample_bytree=0.8, random_state=CONFIG.seed, verbose=-1)\n",
    "lgbm_standalone.fit(X, y)\n",
    "\n",
    "# Cross-validate\n",
    "lgbm_metrics = []\n",
    "for train_idx, valid_idx in tscv.split(X):\n",
    "    x_tr, x_va = X[train_idx], X[valid_idx]\n",
    "    y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "    lgbm_cv = LGBMRegressor(n_estimators=600, learning_rate=0.05, max_depth=-1, subsample=0.8, colsample_bytree=0.8, random_state=CONFIG.seed, verbose=-1)\n",
    "    lgbm_cv.fit(x_tr, y_tr)\n",
    "    preds_va = lgbm_cv.predict(x_va)\n",
    "    rmse = math.sqrt(mean_squared_error(y_va, preds_va))\n",
    "    mae = mean_absolute_error(y_va, preds_va)\n",
    "    r2 = r2_score(y_va, preds_va)\n",
    "    lgbm_metrics.append({\"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "\n",
    "lgbm_metrics_df = pd.DataFrame(lgbm_metrics)\n",
    "lgbm_final_metrics = {\n",
    "    \"model\": \"LightGBM_Standalone\",\n",
    "    \"rmse\": float(lgbm_metrics_df[\"rmse\"].mean()),\n",
    "    \"mae\": float(lgbm_metrics_df[\"mae\"].mean()),\n",
    "    \"r2\": float(lgbm_metrics_df[\"r2\"].mean())\n",
    "}\n",
    "\n",
    "# Forecast with LightGBM\n",
    "pred_log_lgbm = lgbm_standalone.predict(Xf)\n",
    "pred_total_lgbm = np.expm1(pred_log_lgbm)\n",
    "forecast_items[\"pred_lgbm_raw\"] = pred_total_lgbm\n",
    "forecast_items[\"pred_lgbm\"] = forecast_items[\"pred_lgbm_raw\"] * forecast_items[\"growth_factor\"]\n",
    "\n",
    "forecast_daily_lgbm = forecast_items.groupby(forecast_items[\"createdAt\"].dt.date)[\"pred_lgbm\"].sum().reset_index().rename(columns={\"createdAt\": \"date\", \"pred_lgbm\": \"forecast_total_lgbm\"})\n",
    "\n",
    "print(f\"✓ LightGBM Metrics: RMSE={lgbm_final_metrics['rmse']:.4f}, MAE={lgbm_final_metrics['mae']:.4f}, R²={lgbm_final_metrics['r2']:.4f}\")\n",
    "print(f\"✓ LightGBM Forecast (Nov-Dec 2025): €{forecast_daily_lgbm['forecast_total_lgbm'].sum():,.2f}\")\n",
    "lgbm_final_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9181604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Standalone LSTM...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 19 features, but StandardScaler is expecting 25 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     67\u001b[39m lstm_final_metrics = {\n\u001b[32m     68\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLSTM_Standalone\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     69\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(lstm_metrics_df[\u001b[33m\"\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m\"\u001b[39m].mean()),\n\u001b[32m     70\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(lstm_metrics_df[\u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m].mean()),\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(lstm_metrics_df[\u001b[33m\"\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m\"\u001b[39m].mean())\n\u001b[32m     72\u001b[39m }\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Forecast with LSTM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m Xf_scaled = \u001b[43mscaler_lstm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m Xf_lstm = Xf_scaled.reshape((Xf_scaled.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m, Xf_scaled.shape[\u001b[32m1\u001b[39m]))\n\u001b[32m     77\u001b[39m pred_log_lstm = lstm_model.predict(Xf_lstm, verbose=\u001b[32m0\u001b[39m).flatten()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1075\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1072\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1074\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mazhar Iqbal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 19 features, but StandardScaler is expecting 25 features as input."
     ]
    }
   ],
   "source": [
    "# STANDALONE MODEL 3: LSTM (Deep Learning)\n",
    "print(\"Training Standalone LSTM...\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    # Suppress TensorFlow warnings\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    \n",
    "    # Scale features for LSTM\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler_lstm = StandardScaler()\n",
    "    X_scaled = scaler_lstm.fit_transform(X)\n",
    "    \n",
    "    # Reshape for LSTM: (samples, timesteps=1, features)\n",
    "    X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "    \n",
    "    # Build LSTM model\n",
    "    lstm_model = keras.Sequential([\n",
    "        layers.LSTM(128, activation='relu', return_sequences=True, input_shape=(1, X_scaled.shape[1])),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    lstm_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train LSTM with early stopping\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "    lstm_model.fit(X_lstm, y, epochs=100, batch_size=128, verbose=0, callbacks=[early_stop])\n",
    "    \n",
    "    # Cross-validate LSTM\n",
    "    lstm_metrics = []\n",
    "    for train_idx, valid_idx in tscv.split(X):\n",
    "        x_tr, x_va = X[train_idx], X[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "        \n",
    "        scaler_cv = StandardScaler()\n",
    "        x_tr_scaled = scaler_cv.fit_transform(x_tr)\n",
    "        x_va_scaled = scaler_cv.transform(x_va)\n",
    "        \n",
    "        x_tr_lstm = x_tr_scaled.reshape((x_tr_scaled.shape[0], 1, x_tr_scaled.shape[1]))\n",
    "        x_va_lstm = x_va_scaled.reshape((x_va_scaled.shape[0], 1, x_va_scaled.shape[1]))\n",
    "        \n",
    "        lstm_cv = keras.Sequential([\n",
    "            layers.LSTM(128, activation='relu', return_sequences=True, input_shape=(1, x_tr_scaled.shape[1])),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.LSTM(64, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        lstm_cv.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "        lstm_cv.fit(x_tr_lstm, y_tr, epochs=50, batch_size=128, verbose=0)\n",
    "        \n",
    "        preds_va = lstm_cv.predict(x_va_lstm, verbose=0).flatten()\n",
    "        rmse = math.sqrt(mean_squared_error(y_va, preds_va))\n",
    "        mae = mean_absolute_error(y_va, preds_va)\n",
    "        r2 = r2_score(y_va, preds_va)\n",
    "        lstm_metrics.append({\"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "    \n",
    "    lstm_metrics_df = pd.DataFrame(lstm_metrics)\n",
    "    lstm_final_metrics = {\n",
    "        \"model\": \"LSTM_Standalone\",\n",
    "        \"rmse\": float(lstm_metrics_df[\"rmse\"].mean()),\n",
    "        \"mae\": float(lstm_metrics_df[\"mae\"].mean()),\n",
    "        \"r2\": float(lstm_metrics_df[\"r2\"].mean())\n",
    "    }\n",
    "    \n",
    "    # Forecast with LSTM\n",
    "    Xf_scaled = scaler_lstm.transform(Xf)\n",
    "    Xf_lstm = Xf_scaled.reshape((Xf_scaled.shape[0], 1, Xf_scaled.shape[1]))\n",
    "    pred_log_lstm = lstm_model.predict(Xf_lstm, verbose=0).flatten()\n",
    "    pred_total_lstm = np.expm1(pred_log_lstm)\n",
    "    forecast_items[\"pred_lstm_raw\"] = pred_total_lstm\n",
    "    forecast_items[\"pred_lstm\"] = forecast_items[\"pred_lstm_raw\"] * forecast_items[\"growth_factor\"]\n",
    "    \n",
    "    forecast_daily_lstm = forecast_items.groupby(forecast_items[\"createdAt\"].dt.date)[\"pred_lstm\"].sum().reset_index().rename(columns={\"createdAt\": \"date\", \"pred_lstm\": \"forecast_total_lstm\"})\n",
    "    \n",
    "    print(f\"✓ LSTM Metrics: RMSE={lstm_final_metrics['rmse']:.4f}, MAE={lstm_final_metrics['mae']:.4f}, R²={lstm_final_metrics['r2']:.4f}\")\n",
    "    print(f\"✓ LSTM Forecast (Nov-Dec 2025): €{forecast_daily_lstm['forecast_total_lstm'].sum():,.2f}\")\n",
    "    lstm_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ TensorFlow not installed. Skipping LSTM model.\")\n",
    "    print(\"  Install with: pip install tensorflow\")\n",
    "    lstm_final_metrics = {\"model\": \"LSTM_Standalone\", \"rmse\": None, \"mae\": None, \"r2\": None}\n",
    "    lstm_available = False\n",
    "\n",
    "lstm_final_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPARISON SUMMARY\n",
    "comparison_df = pd.DataFrame([\n",
    "    {**metrics, \"model\": \"Stacking_Ensemble\", \"forecast_total\": forecast_daily_stack[\"forecast_total_stack\"].sum()},\n",
    "    {**xgb_final_metrics, \"forecast_total\": forecast_daily_xgb[\"forecast_total_xgb\"].sum()},\n",
    "    {**lgbm_final_metrics, \"forecast_total\": forecast_daily_lgbm[\"forecast_total_lgbm\"].sum()},\n",
    "    {**lstm_final_metrics, \"forecast_total\": forecast_daily_lstm[\"forecast_total_lstm\"].sum() if lstm_available else None}\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 MODEL COMPARISON (Nov-Dec 2025 Forecast)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n🎯 Historical Reference:\")\n",
    "print(f\"  • 2023 Nov-Dec (Mon-Fri): €29,462.42\")\n",
    "print(f\"  • 2024 Nov-Dec (Mon-Fri): €33,869.54\")\n",
    "print(f\"  • Growth Rate (2023→2024): {historical_growth_rate:.2%}\")\n",
    "print(f\"  • Expected 2025 Range: €{33869 * 1.10:,.2f} - €{33869 * 1.20:,.2f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31530d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE ALL ARTIFACTS\n",
    "ensure_output_dirs()\n",
    "\n",
    "# 1. Save cleaned dataset\n",
    "fe.to_csv(os.path.join(CONFIG.output_dir, CONFIG.clean_data_filename), index=False)\n",
    "\n",
    "# 2. Save models\n",
    "model_dir = os.path.join(CONFIG.output_dir, CONFIG.models_dir)\n",
    "dump(stack, os.path.join(model_dir, \"stacking_model.joblib\"))\n",
    "dump(xgb_standalone, os.path.join(model_dir, \"xgb_standalone.joblib\"))\n",
    "dump(lgbm_standalone, os.path.join(model_dir, \"lgbm_standalone.joblib\"))\n",
    "if lstm_available:\n",
    "    lstm_model.save(os.path.join(model_dir, \"lstm_standalone.h5\"))\n",
    "    dump(scaler_lstm, os.path.join(model_dir, \"lstm_scaler.joblib\"))\n",
    "dump(label_encoders, os.path.join(model_dir, \"label_encoders.joblib\"))\n",
    "\n",
    "# 3. Save metrics\n",
    "all_metrics = {\n",
    "    \"stacking_ensemble\": metrics,\n",
    "    \"xgb_standalone\": xgb_final_metrics,\n",
    "    \"lgbm_standalone\": lgbm_final_metrics,\n",
    "    \"lstm_standalone\": lstm_final_metrics,\n",
    "    \"historical_growth_rate\": historical_growth_rate,\n",
    "    \"product_growth_rates\": product_growth\n",
    "}\n",
    "with open(os.path.join(CONFIG.output_dir, CONFIG.metrics_filename), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_metrics, f, indent=2)\n",
    "\n",
    "# 4. Save feature importance\n",
    "importance.to_csv(os.path.join(CONFIG.output_dir, CONFIG.feature_importance_csv), index=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance.head(20), x=\"importance\", y=\"feature\", orient=\"h\")\n",
    "plt.title(\"Top 20 XGBoost Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG.output_dir, CONFIG.feature_importance_png), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 5. Save forecasts\n",
    "forecast_items.to_csv(os.path.join(CONFIG.output_dir, \"forecast_items_nov_dec_2025.csv\"), index=False)\n",
    "\n",
    "# Combine all daily forecasts\n",
    "forecast_daily_combined = forecast_daily_stack.copy()\n",
    "forecast_daily_combined = forecast_daily_combined.merge(forecast_daily_xgb, on=\"date\", how=\"left\")\n",
    "forecast_daily_combined = forecast_daily_combined.merge(forecast_daily_lgbm, on=\"date\", how=\"left\")\n",
    "if lstm_available:\n",
    "    forecast_daily_combined = forecast_daily_combined.merge(forecast_daily_lstm, on=\"date\", how=\"left\")\n",
    "\n",
    "forecast_daily_combined.to_csv(os.path.join(CONFIG.output_dir, CONFIG.forecast_filename), index=False)\n",
    "\n",
    "# 6. Save comparison\n",
    "comparison_df.to_csv(os.path.join(CONFIG.output_dir, \"model_comparison.csv\"), index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ ALL OUTPUTS SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"📁 Output Directory: {CONFIG.output_dir}/\")\n",
    "print(f\"  ├─ {CONFIG.clean_data_filename}\")\n",
    "print(f\"  ├─ {CONFIG.metrics_filename}\")\n",
    "print(f\"  ├─ {CONFIG.feature_importance_csv}\")\n",
    "print(f\"  ├─ {CONFIG.feature_importance_png}\")\n",
    "print(f\"  ├─ {CONFIG.forecast_filename} (all models combined)\")\n",
    "print(f\"  ├─ forecast_items_nov_dec_2025.csv\")\n",
    "print(f\"  ├─ model_comparison.csv\")\n",
    "print(f\"  └─ models/\")\n",
    "print(f\"      ├─ stacking_model.joblib\")\n",
    "print(f\"      ├─ xgb_standalone.joblib\")\n",
    "print(f\"      ├─ lgbm_standalone.joblib\")\n",
    "if lstm_available:\n",
    "    print(f\"      ├─ lstm_standalone.h5\")\n",
    "    print(f\"      ├─ lstm_scaler.joblib\")\n",
    "print(f\"      └─ label_encoders.joblib\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed24d6",
   "metadata": {},
   "source": [
    "---\n",
    "## 📈 RESULTS SUMMARY\n",
    "\n",
    "### **Enhanced Features Implemented:**\n",
    "1. ✅ **Long-term lags**: `lag_90d`, `lag_365d` (capturing seasonal patterns)\n",
    "2. ✅ **Extended rolling means**: `rolling_mean_90d` (trend smoothing)\n",
    "3. ✅ **Revenue growth rate**: YoY product-level growth tracking\n",
    "4. ✅ **Enhanced interactions**: `variant_x_month`, `product_x_weekofyear`\n",
    "5. ✅ **Zero handling**: Replaced with floor value `1e-3` before log transform\n",
    "6. ✅ **Meta-trend corrector**: Product-specific growth factors applied to 2025 forecasts\n",
    "\n",
    "### **Model Architecture:**\n",
    "\n",
    "**🏆 Stacking Ensemble (Primary):**\n",
    "```\n",
    "Base Models → Meta-Learner → Final Forecast + Growth Correction\n",
    "├─ XGBoost (Optuna-tuned)\n",
    "├─ LightGBM (600 estimators)\n",
    "├─ RandomForest (600 estimators)\n",
    "└─ Ridge Regression (meta) + StandardScaler\n",
    "```\n",
    "\n",
    "**🔬 Standalone Models (For Comparison):**\n",
    "- **XGBoost**: Same optimal hyperparameters, independent training\n",
    "- **LightGBM**: Independent gradient boosting\n",
    "- **LSTM**: Deep learning with 2-layer architecture (128→64 units)\n",
    "\n",
    "### **Key Improvements:**\n",
    "- Historical growth (2023→2024) automatically embedded in features\n",
    "- Product-specific growth rates applied to 2025 projections\n",
    "- All models trained with TimeSeriesSplit cross-validation\n",
    "- Meta-trend corrector ensures forecasts follow historical trajectory\n",
    "\n",
    "### **Next Steps:**\n",
    "1. Run all cells sequentially (Cell 1 → Cell 19)\n",
    "2. Compare model performance in the comparison table\n",
    "3. Choose the best-performing model based on:\n",
    "   - Cross-validation metrics (RMSE, MAE, R²)\n",
    "   - Forecast alignment with expected growth trend\n",
    "   - Domain expertise validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ff806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
